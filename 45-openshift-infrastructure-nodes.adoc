[[openshift-infrastructure-nodes]]
= OpenShift Infrastructure Nodes

== OpenShift Infrastructure Nodes

In this section we're going to look at the OpenShift infrastructure nodes, and
more specifically how to scale them once they've been deployed.
To understand what we mean by "infrastructure node", the nodes running the
following services would fall into that description, although this is not
necessarily an exhaustive list:

Kubernetes and OpenShift control plane services ("masters")

- Router(s)
- Container Image Registry
- Cluster metrics collection ("monitoring")
- Cluster aggregated logging
- Service Brokers

NOTE: The OpenShift subscription model allows customers to run various core
infrastructure components at no additional charge. In other words, a node that
is only running core OpenShift infrastructure components is not counted in terms
of the total number of subscriptions required to cover the environment. Any node
running a container, pod, or component not described above is considered a worker
and must be covered by a subscription.

If we wanted to see what different types of nodes, we have in our cluster we need
to do a bit of digging. We can list all of the machines as part of our cluster
in a couple of different ways:

- With the simple `oc get nodes` which queries Kubernetes specifically for the
nodes that are reporting in:

[source,shell]
----
$ oc project default
Now using project "default" on server {{API_URL}}.

$ oc get nodes --show-labels
NAME                                         STATUS   ROLES          AGE   VERSION             LABELS
ip-10-0-128-248.us-east-2.compute.internal   Ready    worker         91m   v1.13.4+c3617b99f   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m4.large,beta.kubernetes.io/os=
linux,failure-domain.beta.kubernetes.io/region=us-east-2,failure-domain.beta.kubernetes.io/zone=us-east-2a,kubernetes.io/hostname=ip-10-0-128-248,node-role.kubernetes.io/worker=,node.opensh
ift.io/os_id=rhcos,node.openshift.io/os_version=4.1
ip-10-0-137-97.us-east-2.compute.internal    Ready    master         26h   v1.13.4+c3617b99f   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m4.xlarge,beta.kubernetes.io/os
=linux,failure-domain.beta.kubernetes.io/region=us-east-2,failure-domain.beta.kubernetes.io/zone=us-east-2a,kubernetes.io/hostname=ip-10-0-137-97,node-role.kubernetes.io/master=,node.opensh
ift.io/os_id=rhcos,node.openshift.io/os_version=4.1
ip-10-0-148-106.us-east-2.compute.internal   Ready    worker         26h   v1.13.4+c3617b99f   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m4.large,beta.kubernetes.io/os=
linux,failure-domain.beta.kubernetes.io/region=us-east-2,failure-domain.beta.kubernetes.io/zone=us-east-2b,kubernetes.io/hostname=ip-10-0-148-106,node-role.kubernetes.io/worker=,node.opensh
ift.io/os_id=rhcos,node.openshift.io/os_version=4.1
ip-10-0-156-129.us-east-2.compute.internal   Ready    master         26h   v1.13.4+c3617b99f   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m4.xlarge,beta.kubernetes.io/os
=linux,failure-domain.beta.kubernetes.io/region=us-east-2,failure-domain.beta.kubernetes.io/zone=us-east-2b,kubernetes.io/hostname=ip-10-0-156-129,node-role.kubernetes.io/master=,node.opens
hift.io/os_id=rhcos,node.openshift.io/os_version=4.1
ip-10-0-167-6.us-east-2.compute.internal     Ready    master         26h   v1.13.4+c3617b99f   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m4.xlarge,beta.kubernetes.io/os
=linux,failure-domain.beta.kubernetes.io/region=us-east-2,failure-domain.beta.kubernetes.io/zone=us-east-2c,kubernetes.io/hostname=ip-10-0-167-6,node-role.kubernetes.io/master=,node.openshi
ft.io/os_id=rhcos,node.openshift.io/os_version=4.1
ip-10-0-170-241.us-east-2.compute.internal   Ready    worker         26h   v1.13.4+c3617b99f   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m4.large,beta.kubernetes.io/os=
linux,failure-domain.beta.kubernetes.io/region=us-east-2,failure-domain.beta.kubernetes.io/zone=us-east-2c,kubernetes.io/hostname=ip-10-0-170-241,node-role.kubernetes.io/worker=,node.opensh
ift.io/os_id=rhcos,node.openshift.io/os_version=4.1
----

- The Machine extension which uses a Kubernetes operator to manage the nodes
themselves through the cluster itself; this can give us a little more information
about the nodes and the underlying infrastructure, noting that this environment
is running on-top of AWS:

[source,shell]
----
$ oc get machines --all-namespaces --show-labels
NAMESPACE               NAME                                         INSTANCE              STATE     TYPE        REGION      ZONE         AGE   LABELS
openshift-machine-api   cluster-3e5f-kqr2b-master-0                  i-02a3979d90b3f67fc   running   m4.xlarge   us-east-2   us-east-2a   26h   machine.openshift.io/cluster-api-cluster=clus
ter-3e5f-kqr2b,machine.openshift.io/cluster-api-machine-role=master,machine.openshift.io/cluster-api-machine-type=master
openshift-machine-api   cluster-3e5f-kqr2b-master-1                  i-051b569b90631f0c7   running   m4.xlarge   us-east-2   us-east-2b   26h   machine.openshift.io/cluster-api-cluster=clus
ter-3e5f-kqr2b,machine.openshift.io/cluster-api-machine-role=master,machine.openshift.io/cluster-api-machine-type=master
openshift-machine-api   cluster-3e5f-kqr2b-master-2                  i-0faba240c04c89423   running   m4.xlarge   us-east-2   us-east-2c   26h   machine.openshift.io/cluster-api-cluster=clus
ter-3e5f-kqr2b,machine.openshift.io/cluster-api-machine-role=master,machine.openshift.io/cluster-api-machine-type=master
openshift-machine-api   cluster-3e5f-kqr2b-worker-us-east-2a-zp8zx   i-087984538c65298ba   running   m4.large    us-east-2   us-east-2a   96m   machine.openshift.io/cluster-api-cluster=clus
ter-3e5f-kqr2b,machine.openshift.io/cluster-api-machine-role=worker,machine.openshift.io/cluster-api-machine-type=worker,machine.openshift.io/cluster-api-machineset=cluster-3e5f-kqr2b-worke
r-us-east-2a
openshift-machine-api   cluster-3e5f-kqr2b-worker-us-east-2b-kbdvm   i-0b401fa18f126f628   running   m4.large    us-east-2   us-east-2b   26h   machine.openshift.io/cluster-api-cluster=clus
ter-3e5f-kqr2b,machine.openshift.io/cluster-api-machine-role=worker,machine.openshift.io/cluster-api-machine-type=worker,machine.openshift.io/cluster-api-machineset=cluster-3e5f-kqr2b-worke
r-us-east-2b
openshift-machine-api   cluster-3e5f-kqr2b-worker-us-east-2c-m9zgh   i-0314b11c4c7d0a89f   running   m4.large    us-east-2   us-east-2c   26h   machine.openshift.io/cluster-api-cluster=clus
ter-3e5f-kqr2b,machine.openshift.io/cluster-api-machine-role=worker,machine.openshift.io/cluster-api-machine-type=worker,machine.openshift.io/cluster-api-machineset=cluster-3e5f-kqr2b-worke
r-us-east-2c
----

- In both of these outputs you'll note that both of the outputs have roles listed,
and these are associated to kubernetes labels for scheduling purposes; we have
two types of role defined: `master` and `worker`. With the exception of master
nodes (due to specific scheduling and deployment limitations), all other node
types are deployed as part of a `MachineSet`, and can therefore be scaled as one.

=== More MachineSet Details

- In the previous section, you explored the MachineSets resource and scaled the
cluster by changing its replica count, adding additional workers, we also
configured an auto scaler to ensure that the cluster would have the required
capacity to accommodate the workload demand. You'll note that right now we only
have worker type machine sets configured:

[source,shell]
----
$ oc get machinesets -n openshift-machine-api
NAME                                   DESIRED   CURRENT   READY   AVAILABLE   AGE
cluster-3e5f-kqr2b-worker-us-east-2a   1         1         1       1           26h
cluster-3e5f-kqr2b-worker-us-east-2b   1         1         1       1           26h
cluster-3e5f-kqr2b-worker-us-east-2c   1         1         1       1           26h
----

- Here we have three distinct machine sets deployed, each with a single machine
running. Each machine set is aligned with a given AWS EC2 availability zone,
and we could easily deploy additional machines (or nodes) into one of the three
listed (2a, 2b and 2c). If we wanted to add additional capacity to our cluster
we could adjust the replica count like we did in the previous lab, remembering t
hat we randomly selected one of the machine sets in the overall configuration,
but that's just for worker nodes, i.e. where our user applications run, and where
some of the key infrastructure components run by default, e.g. our routers, etc.

- If we wanted to create a dedicated infrastructure role where we could run
specific infrastructure services, components, and pods on (and move them away
from the worker nodes like they are by default). We would need to:
  1. Create an additional set of nodes
  2. Define a MachineSet to deploy and scale them into
  3. Label them with specific kubernetes labels.
  4. Configure the various components to run specifically on nodes with those labels.

- To accomplish this, you will create additional MachineSets. The easiest way to do this is:
  1. Get the existing MachineSets by downloading it into a file
  2. Modifying them. This is because the MachineSet has some details that are
  specific to the AWS region that the cluster is deployed in, like the AWS EC2
  AMI ID, so crafting it by hand would be very difficult.

- Here is what we are going to do:
  1. Take a look at one of our MachineSets in detail to understand how the configuration is set
  2. Look to adapt it to create a new one for specifically for our infrastructure services.
  3. Use the following command, noting that you'll have to adjust the command to
  suit the name of your machine set. You can pick one of the machine sets from previous command:

[source,shell]
----
[~] $ oc project openshift-machine-api
[~] $ oc get machineset cluster-3e5f-kqr2b-worker-us-east-2a -o yaml
----

NOTE: Name of machineset is from your environment will be different from this above command.

- Which will give you the following output:

[source,shell]
----
[~] $ oc get machineset cluster-3e5f-kqr2b-worker-us-east-2a -o yaml

apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  annotations:
    autoscaling.openshift.io/machineautoscaler: openshift-machine-api/autoscale-us-east-2a-ts7rr
    machine.openshift.io/cluster-api-autoscaler-node-group-max-size: "4"
    machine.openshift.io/cluster-api-autoscaler-node-group-min-size: "1"
  creationTimestamp: "2019-05-13T20:34:26Z"
  generation: 9
  labels:
    machine.openshift.io/cluster-api-cluster: cluster-3e5f-kqr2b
  name: cluster-3e5f-kqr2b-worker-us-east-2a
  namespace: openshift-machine-api
  resourceVersion: "446823"
  selfLink: /apis/machine.openshift.io/v1beta1/namespaces/openshift-machine-api/machinesets/cluster-3e5f-kqr2b-worker-us-east-2a
  uid: 80644a16-75be-11e9-bb7c-02f7ee4a116e
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: cluster-3e5f-kqr2b
      machine.openshift.io/cluster-api-machine-role: worker
      machine.openshift.io/cluster-api-machine-type: worker
      machine.openshift.io/cluster-api-machineset: cluster-3e5f-kqr2b-worker-us-east-2a
  template:
    metadata:
----

There are a few very important sections in the output, we'll discuss them in depth below...

==== Metadata

The metadata on the MachineSet itself includes information like the name of the MachineSet and various labels:

[source,shell]
----
metadata:
  annotations:
    autoscaling.openshift.io/machineautoscaler: openshift-machine-api/autoscale-us-east-2a-ts7rr
    machine.openshift.io/cluster-api-autoscaler-node-group-max-size: "4"
    machine.openshift.io/cluster-api-autoscaler-node-group-min-size: "1"
  creationTimestamp: "2019-05-13T20:34:26Z"
  generation: 9
  labels:
    machine.openshift.io/cluster-api-cluster: cluster-3e5f-kqr2b
  name: cluster-3e5f-kqr2b-worker-us-east-2a
  namespace: openshift-machine-api
  resourceVersion: "446823"
  selfLink: /apis/machine.openshift.io/v1beta1/namespaces/openshift-machine-api/machinesets/cluster-3e5f-kqr2b-worker-us-east-2a
  uid: 80644a16-75be-11e9-bb7c-02f7ee4a116e
----


NOTE: You might see some annotations on your MachineSet if you use the MachineSet that you defined a MachineAutoScaler on in the previous lab section.

==== Selector

[source,shell]
----
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: cluster-3e5f-kqr2b
      machine.openshift.io/cluster-api-machine-role: worker
      machine.openshift.io/cluster-api-machine-type: worker
      machine.openshift.io/cluster-api-machineset: cluster-3e5f-kqr2b-worker-us-east-2a
----

In this case, the cluster name is 3e5f-kqr2b and there is an additional label for the whole set.

==== Template Metadata

The template section is the part of the MachineSet that specifically templates out the Machine. The template itself can have metadata associated, and we need to make sure that things match here when we make changes:

[source,shell]
----
template:
    metadata:
      creationTimestamp: null
      labels:
        machine.openshift.io/cluster-api-cluster: cluster-3e5f-kqr2b
        machine.openshift.io/cluster-api-machine-role: worker
        machine.openshift.io/cluster-api-machine-type: worker
        machine.openshift.io/cluster-api-machineset: cluster-3e5f-kqr2b-worker-us-east-2a
----

==== Template Spec

The template needs to specify how the Machine/node should be created, i.e. "use this configuration for all machines in this set"; this configuration will be used when provisioning new systems when scaling is required. You will notice that the spec and, more specifically, the providerSpec contains all of the important AWS data to help get the Machine created correctly and bootstrapped.

In our case, we want to ensure that the resulting node inherits one or more specific labels. As you've seen in the examples above, labels go in metadata sections:

[source,shell]
----
spec:
      metadata:
        creationTimestamp: null
      providerSpec:
        value:
          ami:
            id: ami-02200f690a88f0819
          apiVersion: awsproviderconfig.openshift.io/v1beta1
          blockDevices:
          - ebs:
              iops: 0
              volumeSize: 120
              volumeType: gp2
          credentialsSecret:
            name: aws-cloud-credentials
          deviceIndex: 0
          iamInstanceProfile:
            id: cluster-3e5f-kqr2b-worker-profile
          instanceType: m4.large
          kind: AWSMachineProviderConfig
          metadata:
            creationTimestamp: null
          placement:
            availabilityZone: us-east-2a
            region: us-east-2
          publicIp: null
          securityGroups:
          - filters:
            - name: tag:Name
              values:
              - cluster-3e5f-kqr2b-worker-sg
          subnet:
            filters:
            - name: tag:Name
              values:
              - cluster-3e5f-kqr2b-private-us-east-2a
          tags:
          - name: kubernetes.io/cluster/cluster-3e5f-kqr2b
            value: owned
          userDataSecret:
            name: worker-user-data
----

By default the MachineSets that the installer creates do not apply any additional labels to the node.

NOTE: As you can probably see, there's plenty of AWS-specific provider configuration here, in future versions of OpenShift, there will be similar respective parameters for other infrastructure providers that can be used.

==== Defining a Custom MachineSet

In this section we're going to be defining a custom MachineSet for infrastructure services. Now that you've inspected an existing MachineSet it's time to go over the rules for creating one, at least for a simple change like we're making:

- Don't change anything in the providerSpec
- Don't change any instances of sigs.k8s.io/cluster-api-cluster: <clusterid>
- Give your MachineSet a unique name
- Make sure any instances of sigs.k8s.io/cluster-api-machineset match the name
- Add labels you want on the nodes to .spec.template.spec.metadata.labels
- Even though you're changing MachineSet name references, be sure not to change the subnet.

This sounds complicated, so let's go through an example. Go ahead and dump one of your existing MachineSets to a file, remembering to adjust this command to match one of yours:

[source,shell]
----
$ oc get machineset cluster-3e5f-kqr2b-worker-us-east-2a -o yaml -n openshift-machine-api > infra-machineset.yaml
----

NOTE: Name of machineset is from your environment will be different from this above command.

Now open it with a text editor of your choice:

[source,shell]
----
$ vi infra-machineset.yaml
----

Let's now take some steps to adapt this MachineSet to suit our required new infrastructure node type...

==== Clean it

Since we asked OpenShift to tell us about an existing MachineSet, there's a lot of extra data that we can immediately remove from the file. Remove the following:

- Within the .metadata top level, remove:

    * generation
    * resourceVersion
    * selfLink
    * uid

- The entire .status block.

- All instances of creationTimestamp.

==== Name It

- Change the top-level .metadata.name to something indicative of the purpose of
this set, for example:

[source,shell]
----
name: infrastructure-ap-east-2a
(or anything you name it)
----

- Search for `-machineset` (only 2 places), we can tell that it houses
infrastructure-focused Machines (nodes) in us-east-2 region in the availability
zone. Ultimately, you can call this anything you like, but we should change this
to something that makes sense for your cluster.

==== Match It
- Change any instance of sigs.k8s.io/cluster-api-machineset to match your new name
of infrastructure-us-east-2a (or whatever you're using). This appears in both
.spec.selector.matchLabels as well as .spec.template.metadata.labels.

===== Add Your Node Label

- Add a labels section to .spec.template.spec.metadata with the label
node-role.kubernetes.io/infra: "". Because `oc get node` looks at the
node-role.kubernetes.io/xxx label and shows that in the output. This will make
it easy to identify which workers are also infrastructure nodes.

- Your resulting section should look somewhat like the following, albeit with
slightly different names as per your unique cluster name:

[source,shell]
----
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: cluster-3e5f-kqr2b
      machine.openshift.io/cluster-api-machine-role: worker
      machine.openshift.io/cluster-api-machine-type: worker
      machine.openshift.io/cluster-api-machineset: infrastructure-us-east-2a
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: cluster-3e5f-kqr2b
        machine.openshift.io/cluster-api-machine-role: worker
        machine.openshift.io/cluster-api-machine-type: worker
        machine.openshift.io/cluster-api-machineset: infrastructure-us-east-2a
    spec:
      metadata:
        labels:
          node-role.kubernetes.io/infra: ""
----

==== Set the replica count

- Set the replica count 1

[source,shell]
----
spec:
  replicas: 1
----

==== Change the Instance Type

- If you want a different EC2 instance type, you can change that. It is one of
the few things in the providerSpec block you can realistically change. You can
also change volumes if you want a different storage size or need additional
volumes on your instances.

- Save your file and exit.

==== Double Check

- Your cluster will have a different ID and you are likely operating in a
different version, however, your file should more or less look like the following:

- Here is an example of a working infra-machineset.yaml:

[source,shell]
----
[~] $ cat infra-machineset.yaml

apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: cluster-4c7b-lkw4d
  name: infra-us-east-2a
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: cluster-4c7b-lkw4d
      machine.openshift.io/cluster-api-machine-role: worker
      machine.openshift.io/cluster-api-machine-type: worker
      machine.openshift.io/cluster-api-machineset: infra-us-east-2a
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: cluster-4c7b-lkw4d
        machine.openshift.io/cluster-api-machine-role: worker
        machine.openshift.io/cluster-api-machine-type: worker
        machine.openshift.io/cluster-api-machineset: infra-us-east-2a
    spec:
      metadata:
        labels:
          node-role.kubernetes.io/infra: ""
      providerSpec:
        value:
          ami:
            id: ami-02200f690a88f0819
          apiVersion: awsproviderconfig.openshift.io/v1beta1
          blockDevices:
          - ebs:
              iops: 0
              volumeSize: 120
              volumeType: gp2
          credentialsSecret:
            name: aws-cloud-credentials
          deviceIndex: 0
          iamInstanceProfile:
            id: cluster-4c7b-lkw4d-worker-profile
          instanceType: m4.large
          kind: AWSMachineProviderConfig
          metadata:
            creationTimestamp: null
          placement:
            availabilityZone: us-east-2a
            region: us-east-2
          publicIp: null
          securityGroups:
          - filters:
            - name: tag:Name
              values:
              - cluster-4c7b-lkw4d-worker-sg
          subnet:
            filters:
            - name: tag:Name
              values:
              - cluster-4c7b-lkw4d-private-us-east-2a
          tags:
          - name: kubernetes.io/cluster/cluster-4c7b-lkw4d
            value: owned
          userDataSecret:
            name: worker-user-data
      versions:
        kubelet: ""
----

=== Create Your Machineset

- Now you can create your MachineSet from the definition that we created:

[source,shell]
----
$ oc create -f infra-machineset.yaml -n openshift-machine-api
----

- Then go ahead and check to see if this new MachineSet is listed:

[source,shell]
----
[~] $ oc get machineset -n openshift-machine-api
NAME                                   DESIRED   CURRENT   READY   AVAILABLE   AGE
cluster-3e5f-kqr2b-worker-us-east-2a   1         1         1       1           32h
cluster-3e5f-kqr2b-worker-us-east-2b   1         1         1       1           32h
cluster-3e5f-kqr2b-worker-us-east-2c   1         1         1       1           32h
infrastructure-ap-east-2a              1         1                             46s
----

- We don't yet have any ready or available machines in the set because the
instance is still coming up and bootstrapping. We can check every minute or to
see see whether the machine has been created or not, noting that in the output
below the new node is now running:

[source,shell]
----
$ oc get machine -n openshift-machine-api
NAME                                         INSTANCE              STATE     TYPE        REGION      ZONE         AGE
cluster-3e5f-kqr2b-master-0                  i-02a3979d90b3f67fc   running   m4.xlarge   us-east-2   us-east-2a   32h
cluster-3e5f-kqr2b-master-1                  i-051b569b90631f0c7   running   m4.xlarge   us-east-2   us-east-2b   32h
cluster-3e5f-kqr2b-master-2                  i-0faba240c04c89423   running   m4.xlarge   us-east-2   us-east-2c   32h
cluster-3e5f-kqr2b-worker-us-east-2a-zp8zx   i-087984538c65298ba   running   m4.large    us-east-2   us-east-2a   7h46m
cluster-3e5f-kqr2b-worker-us-east-2b-kbdvm   i-0b401fa18f126f628   running   m4.large    us-east-2   us-east-2b   32h
cluster-3e5f-kqr2b-worker-us-east-2c-m9zgh   i-0314b11c4c7d0a89f   running   m4.large    us-east-2   us-east-2c   32h
infrastructure-ap-east-2a-2swqt              i-0c68084ced1b9427b   running   m4.large    us-east-2   us-east-2a   20h
----

- Now we can use oc get nodes to see when the actual node is joined and ready.
If you're having trouble figuring out which node is the new one, take a look at
the AGE column. It will be the youngest! Again, this node may show up as a
Machine in the previous API call, but may not have joined the cluster yet, so
give it some time to bootstrap properly.

[source,shell]
----
$ oc get nodes
NAME                                         STATUS   ROLES          AGE     VERSION
ip-10-0-128-248.us-east-2.compute.internal   Ready    worker         7h46m   v1.13.4+c3617b99f
ip-10-0-137-106.us-east-2.compute.internal   Ready    infra,worker   20h     v1.13.4+c3617b99f
ip-10-0-137-97.us-east-2.compute.internal    Ready    master         32h     v1.13.4+c3617b99f
ip-10-0-148-106.us-east-2.compute.internal   Ready    worker         32h     v1.13.4+c3617b99f
ip-10-0-156-129.us-east-2.compute.internal   Ready    master         32h     v1.13.4+c3617b99f
ip-10-0-167-6.us-east-2.compute.internal     Ready    master         32h     v1.13.4+c3617b99f
ip-10-0-170-241.us-east-2.compute.internal   Ready    worker         32h     v1.13.4+c3617b99f
----

==== Check the Labels

- In our case, the youngest node was named ip-10-0-137-106.us-east-2.compute.internal, so we can ask what its labels are:

[source,shell]
----
$ oc get node ip-10-0-137-106.us-east-2.compute.internal --show-labels
NAME                                         STATUS   ROLES          AGE   VERSION             LABELS
ip-10-0-137-106.us-east-2.compute.internal   Ready    infra,worker   20h   v1.13.4+c3617b99f   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m4.large,
beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=us-east-2,failure-domain.beta.kubernetes.io/zone=us-east-2a,kubernetes.io/hostname=ip-10-0-137-106
,node-role.kubernetes.io/infra=,node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos,node.openshift.io/os_version=4.1
----

- It's hard to see, but our `node-role.kubernetes.io/infra` label is the LABELS
column. You will also see infra,worker in the output of oc get node in the ROLES
column. Success!

==== Add More Machinesets (or scale, or both)

- In a realistic production deployment, you would want at least 3 MachineSets to
hold infrastructure components. Both the logging aggregation solution and the
service mesh will deploy ElasticSearch, and ElasticSearch really needs 3 instances
spread across 3 discrete nodes. Why 3 MachineSets? Well, in theory, having a
MachineSet in different AZs ensures that you don't go completely dark if AWS
loses an AZ. For the purposes of this exercise, though, we'll just scale up our
single set:

[source,shell]
----
$ oc edit machineset infrastructure-us-east-2a -n openshift-machine-api
(Opens in vi)
----

NOTE: If you're uncomfortable with vi(m) you can use your favorite editor by
specifying EDITOR=<your choice> before the oc command.

- Change the .spec.replicas from 1 to 3, and then save/exit the editor.

- Execute the following commands to examine the environment:
  1. `oc project openshift-machine-api`
  2. `oc get machineset` to see the change in the desired number of instances
  3. `oc get machine`
  4. `oc get node`

- Two more infra machines will be created.

=== Moving Infrastructure Components

Now that we have provisioned some infrastructure specific nodes, it's time to
move various infrastructure components onto them, i.e. move them away from the
worker nodes, and onto the fresh systems. Let's go through some of them
individually to see how they can be moved, and how to monitor the progress.

==== Router

- The OpenShift router is deployed, maintained, and scaled by an Operator called
openshift-ingress-operator. Its Pod lives in the openshift-ingress-operator project:

[source,shell]
----
$ oc get pod -n openshift-ingress-operator
NAME                               READY   STATUS    RESTARTS   AGE
ingress-operator-5895456c5-vwnc6   1/1     Running   0          32h
----

- The actual default router instance lives in the openshift-ingress project:

[source,shell]
----
$ oc get pod -n openshift-ingress -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP           NODE                                         NOMINATED NODE   READINESS GATES
router-default-7db478d879-bzwws   1/1     Running   0          20h   10.131.4.4   ip-10-0-128-248.us-east-2.compute.internal   <none>           <none>
router-default-7db478d879-nwftw   1/1     Running   0          20h   10.130.4.4   ip-10-0-170-241.us-east-2.compute.internal   <none>           <none>
----

- The cluster deploys two routers for availability and fault tolerance, and you
can see that the pods are deployed across two nodes. Right now, these will be
deployed on nodes with the worker label, and not on the infrastructure nodes that
were recently deployed, as the default configuration of the router operator is
to pick nodes with the role of worker.

- Pick one of the nodes (from NODE) where a router pod is running and see the
ROLES column:

[source,shell]
----
$ oc get node ip-10-0-170-241.us-east-2.compute.internal
NAME                                         STATUS   ROLES    AGE   VERSION
ip-10-0-170-241.us-east-2.compute.internal   Ready    worker   32h   v1.13.4+c3617b99f
----

- now that we have created dedicated infrastructure nodes, we want to tell the
operator to put the router instances on nodes with the new role of infra.

- The OpenShift router operator creates a custom resource definition (CRD) called
ingresscontroller. The ingresscontroller objects are observed by the router
operator and tell the operator how to create and configure routers.
Let's take a look:

[source,shell]
----
$ oc get ingresscontroller default -n openshift-ingress-operator -o yaml

apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: "2019-05-13T20:39:27Z"
  finalizers:
  - ingresscontroller.operator.openshift.io/finalizer-ingresscontroller
  generation: 2
  name: default
  namespace: openshift-ingress-operator
  resourceVersion: "199439"
  selfLink: /apis/operator.openshift.io/v1/namespaces/openshift-ingress-operator/ingresscontrollers/default
  uid: 33c90a62-75bf-11e9-a65b-02affe1c7e26
spec:
  nodePlacement:
    nodeSelector:
      matchLabels:
        node-role.kubernetes.io/worker: ""
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: "2019-05-13T20:41:27Z"
    status: "True"
    type: Available
  domain: apps.cluster-3e5f.sandbox580.opentlc.com
  endpointPublishingStrategy:
    type: LoadBalancerService
  selector: ingresscontroller.operator.openshift.io/deployment-ingresscontroller=default
----

- As you can see, the nodeSelector is configured for the worker role. Go ahead and use oc edit to change node-role.kubernetes.io/worker to be node-role.kubernetes.io/infra:

[source,shell]
----
$ oc edit ingresscontroller default -n openshift-ingress-operator -o yaml
(Opens in vi)
----

- The relevant section should look like the following:


[source,shell]
----
spec:
  nodePlacement:
    nodeSelector:
      matchLabels:
        node-role.kubernetes.io/infra: ""

----

- After saving and exiting the editor, if you're quick enough, you might catch the router pod being moved to its new home. Run the following command and you may see something like:

[source,shell]
----
$ oc get pod -n openshift-ingress -o wide
NAME                              READY     STATUS        RESTARTS   AGE       IP           NODE                                              NOMINATED NODE
router-default-5fc6c9ffbb-9x9l8   1/1       Running       0          15h       10.131.4.4   ip-10-0-139-255.us-east-2.compute.internal        <none>
router-default-5fc6c9ffbb-p5x6d   0/1       Terminating   0          15h       10.131.4.4   ip-10-0-128-248.us-east-2.compute.internal        <none>
----

- In the above output, the Terminating pod was running on one of the worker nodes.
The Running pod is now on one of our nodes


WARNING: If you're using the browser-based terminal, your session will hang when
the router pods get removed, as we're reliant on the routers to serve your console.
The session may restore itself after a minute or two, or you can try reloading
the page.

- If we wait a minute or so, we should see that the pods are rebuilt:

[source,shell]
----
$ oc get pod -n openshift-ingress -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP           NODE                                         NOMINATED NODE   READINESS GATES
router-default-7db478d879-bzwws   1/1     Running   0          22h   10.131.4.4   ip-10-0-139-255.us-east-2.compute.internal   <none>           <none>
router-default-7db478d879-nwftw   1/1     Running   0          22h   10.130.4.4   ip-10-0-137-106.us-east-2.compute.internal   <none>           <none>
----

- If we check one of the nodes for the ROLE that it's labeled with:

[source,shell]
----
$ oc get node ip-10-0-139-255.us-east-2.compute.internal
NAME                                         STATUS   ROLES          AGE   VERSION
ip-10-0-139-255.us-east-2.compute.internal   Ready    infra,worker   22h   v1.13.4+c3617b99f
----

Success! Our pods have been automatically redeployed onto the infrastructure nodes.

==== Container Image Registry

The registry uses a similar CRD (Custom Resource Definition) mechanism to
configure how the operator deploys the actual registry pods. That CRD is
configs.imageregistry.operator.openshift.io. You will need to edit the cluster
CR object in order to add the nodeSelector.

- First, take a look at it:

[source,shell]
----
[~] $ oc get configs.imageregistry.operator.openshift.io/cluster -o yaml

apiVersion: imageregistry.operator.openshift.io/v1
kind: Config
metadata:
  creationTimestamp: "2019-05-13T20:39:22Z"
  finalizers:
  - imageregistry.operator.openshift.io/finalizer
  generation: 3
  name: cluster
  resourceVersion: "200927"
  selfLink: /apis/imageregistry.operator.openshift.io/v1/configs/cluster
  uid: 3077588d-75bf-11e9-8ad1-0af01fb55bd2
spec:
  defaultRoute: false
  httpSecret: 66b879954287368617ed5165caff19ebd07d2dabe4edb84509875623b9ff07914de72f832d4e80bb993d18220e935a65ce3b30e29eaf170f645b2d2e4a65a2c0
  logging: 2
  managementState: Managed
  proxy:
    http: ""
    https: ""
    noProxy: ""
  readOnly: false
  replicas: 1
  requests:
    read:
      maxInQueue: 0
      maxRunning: 0
      maxWaitInQueue: 0s
    write:
      maxInQueue: 0
      maxRunning: 0
      maxWaitInQueue: 0s
(...)
----

- Next, let's modify the custom resource by live-patching the configuration.
For this we can use oc edit, and you'll need to modify the .spec section:

[source,shell]
----
[~] $ oc edit configs.imageregistry.operator.openshift.io/cluster
----

- The .spec section will need to look like the following:

[source,shell]
----
  nodeSelector:
    node-role.kubernetes.io/infra: ""
----

- Once you're done, save and exit the editor, and it should confirm the change:

[source,shell]
----
config.imageregistry.operator.openshift.io/cluster edited
----

NOTE: The nodeSelector stanza may be added anywhere inside the .spec block.

- When you save and exit you should see the registry pod being moved to the infra
node. The registry is in the openshift-image-registry project. If you execute
the following quickly enough, you may see the old registry pods terminating and
the new ones starting.:

[source,shell]
----
[~] $ oc get pod -n openshift-image-registry
NAME                                               READY   STATUS        RESTARTS   AGE
cluster-image-registry-operator-5644775d7c-w78kh   1/1     Running       0          34h
image-registry-5878c9d896-nmkc6                    1/1     Terminating   0          22h
node-ca-2ljck                                      1/1     Running       0          22h
node-ca-9npbz                                      1/1     Running       0          34h
node-ca-mk9lj                                      1/1     Running       0          34h
node-ca-pspwx                                      1/1     Running       0          34h
node-ca-qlxqx                                      1/1     Running       0          9h
node-ca-qvslw                                      1/1     Running       0          34h
node-ca-wxb55                                      1/1     Running       0          34h
node-ca-xn9vg                                      1/1     Running       0          22h
----

NOTE: At this time the image registry is not using a separate project for its operator. Both the operator and the operand are housed in the openshift-image-registry project.

- Since the registry is being backed by an S3 bucket, it doesn't matter what node the new registry pod instance lands on. It's talking to an object store via an API, so any existing images stored there will remain accessible.

- Also note that the default replica count is 1. In a real-world environment you might wish to scale that up for better availability, network throughput, or other reasons.

- If you look at the node on which the registry landed (noting that you'll likely have to refresh your list of pods by using the previous commands to get its new name):

[source,shell]
----
[~] $ oc get pod image-registry-5878c9d896-nmkc6 -n openshift-image-registry -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP           NODE                                         NOMINATED NODE   READINESS GATES
image-registry-5878c9d896-nmkc6   1/1     Running   0          22h   10.131.4.5   ip-10-0-139-255.us-east-2.compute.internal   <none>           <none>
----

NOTE: the pod name will be different in your environment

- it is now running on an infra worker:

[source,shell]
----
[~] $ oc get node ip-10-0-139-255.us-east-2.compute.internal
NAME                                         STATUS   ROLES          AGE   VERSION
ip-10-0-139-255.us-east-2.compute.internal   Ready    infra,worker   22h   v1.13.4+c3617b99f
----

- Notice that the CRD for the image registry's configuration is not
namespaced -- it is cluster scoped. There is only one internal/integrated
registry per OpenShift cluster that serves all projects.

==== Monitoring

The Cluster Monitoring operator is responsible for deploying and managing the
state of the Prometheus+Grafana+AlertManager cluster monitoring stack. It is
installed by default during the initial cluster installation. Its operator uses
a ConfigMap in the openshift-monitoring project to set various tunables and
settings for the behavior of the monitoring stack.

- There is no ConfigMap created as part of the installation. Without one, the
operator will assume default settings, as we can see, this is not defined:

[source,shell]
----
[~] $ oc get configmap cluster-monitoring-config -n openshift-monitoring
Error from server (NotFound): configmaps "cluster-monitoring-config" not found
----

- Even with the default settings, The operator will create several ConfigMap objects for the various monitoring stack components, and you can see them, too:

[source,shell]
----
[~] $ oc get configmap -n openshift-monitoring
NAME                                                  DATA   AGE
adapter-config                                        1      34h
grafana-dashboard-etcd                                1      34h
grafana-dashboard-k8s-cluster-rsrc-use                1      34h
grafana-dashboard-k8s-node-rsrc-use                   1      34h
grafana-dashboard-k8s-resources-cluster               1      34h
grafana-dashboard-k8s-resources-namespace             1      34h
grafana-dashboard-k8s-resources-pod                   1      34h
grafana-dashboard-k8s-resources-workload              1      34h
grafana-dashboard-k8s-resources-workloads-namespace   1      34h
grafana-dashboards                                    1      34h
kubelet-serving-ca-bundle                             1      34h
prometheus-adapter-prometheus-config                  1      34h
prometheus-k8s-rulefiles-0                            1      34h
serving-certs-ca-bundle                               1      34h
sharing-config                                        3      34h
telemeter-client-serving-certs-ca-bundle              1      34h
----

- Take a look at the following file, it contains the definition for a ConfigMap
that will cause the monitoring solution to be redeployed onto infrastructure nodes:

https://github.com/openshift/training/blob/master/assets/cluster-monitoring-configmap.yaml

- Let's use this as our new configuration; you can create the new monitoring
config with the following command:

[source,shell]
----
[~] $ oc create -f https://raw.githubusercontent.com/openshift/training/master/assets/cluster-monitoring-configmap.yaml
configmap/cluster-monitoring-config created
----

- We can now watch the various monitoring pods be redeployed onto our
infrastructure nodes with the following command:

[source,shell]
----
[~] $ oc get pod -w -n openshift-monitoring
NAME                                           READY     STATUS              RESTARTS   AGE
alertmanager-main-0                            3/3       Running             0          16h
alertmanager-main-1                            3/3       Running             0          16h
alertmanager-main-2                            0/3       ContainerCreating   0          3s
cluster-monitoring-operator-6fc8c9bc75-6pfpw   1/1       Running             0          16h
grafana-574679769d-7f9mf                       2/2       Running             0          16h
kube-state-metrics-55f8d66c77-sbbbc            3/3       Running             0          16h
kube-state-metrics-578dbdf85d-85vm7            0/3       ContainerCreating   0          9s
node-exporter-2x7b7                            2/2       Running             0          16h
node-exporter-d4vq9                            2/2       Running             0          45m
node-exporter-dx5kz                            2/2       Running             0          16h
node-exporter-f9g4h                            2/2       Running             0          16h
node-exporter-kvd5x                            2/2       Running             0          45m
node-exporter-ntzbp                            2/2       Running             0          16h
node-exporter-prsj9                            2/2       Running             0          1h
node-exporter-qx9lf                            2/2       Running             0          16h
node-exporter-wh9qs                            2/2       Running             0          16h
prometheus-adapter-7fb8c8b544-jn8q2            1/1       Running             0          32m
prometheus-adapter-7fb8c8b544-v5rfs            1/1       Running             0          33m
prometheus-k8s-0                               6/6       Running             1          16h
prometheus-k8s-1                               6/6       Running             1          16h
prometheus-operator-7787679668-nxc6s           0/1       ContainerCreating   0          8s
prometheus-operator-954644495-m64hd            1/1       Running             0          16h
telemeter-client-79f99d7bc6-4p8zv              3/3       Running             0          16h
telemeter-client-7f48f48dd7-dvblb              0/3       ContainerCreating   0          4s
grafana-5fc5979587-bdkcd                       0/2       Pending             0          3s

(Ctrl+C to exit)
----

NOTE: You can also run watch 'oc get pod -n openshift-monitoring' as an alternative.

Congratulations!! You now know how to set up infrastructure nodes on OpenShift 4 cluster!! For more information, see https://docs.openshift.com/container-platform/4.1/machine_management/creating-infrastructure-machinesets.html.
