[[openshift-infrastructure-nodes]]
= OpenShift Infrastructure Nodes

== OpenShift Infrastructure Nodes

In this section we're going to look at the OpenShift infrastructure nodes, and more specifically how to scale them once they've been deployed. To understand what we mean by "infrastructure node", the nodes running the following services would fall into that description, although this is not necessarily an exhaustive list:

Kubernetes and OpenShift control plane services ("masters")

- Router(s)
- Container Image Registry
- Cluster metrics collection ("monitoring")
- Cluster aggregated logging
- Service Brokers

NOTE: The OpenShift subscription model allows customers to run various core infrastructure components at no additional charge. In other words, a node that is only running core OpenShift infrastructure components is not counted in terms of the total number of subscriptions required to cover the environment. Any node running a container, pod, or component not described above is considered a worker and must be covered by a subscription.

If we wanted to see what different types of nodes, we have in our cluster we need to do a bit of digging.
We can list all of the machines as part of our cluster in a couple of different ways:

- With the simple oc get nodes which queries Kubernetes specifically for the nodes that are reporting in:

```
$ oc project default
Now using project "default" on server {{API_URL}}.

$ oc get nodes --show-labels
NAME                                         STATUS   ROLES          AGE   VERSION             LABELS
ip-10-0-128-248.us-east-2.compute.internal   Ready    worker         91m   v1.13.4+c3617b99f   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m4.large,beta.kubernetes.io/os=
linux,failure-domain.beta.kubernetes.io/region=us-east-2,failure-domain.beta.kubernetes.io/zone=us-east-2a,kubernetes.io/hostname=ip-10-0-128-248,node-role.kubernetes.io/worker=,node.opensh
ift.io/os_id=rhcos,node.openshift.io/os_version=4.1
ip-10-0-137-97.us-east-2.compute.internal    Ready    master         26h   v1.13.4+c3617b99f   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m4.xlarge,beta.kubernetes.io/os
=linux,failure-domain.beta.kubernetes.io/region=us-east-2,failure-domain.beta.kubernetes.io/zone=us-east-2a,kubernetes.io/hostname=ip-10-0-137-97,node-role.kubernetes.io/master=,node.opensh
ift.io/os_id=rhcos,node.openshift.io/os_version=4.1
ip-10-0-148-106.us-east-2.compute.internal   Ready    worker         26h   v1.13.4+c3617b99f   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m4.large,beta.kubernetes.io/os=
linux,failure-domain.beta.kubernetes.io/region=us-east-2,failure-domain.beta.kubernetes.io/zone=us-east-2b,kubernetes.io/hostname=ip-10-0-148-106,node-role.kubernetes.io/worker=,node.opensh
ift.io/os_id=rhcos,node.openshift.io/os_version=4.1
ip-10-0-156-129.us-east-2.compute.internal   Ready    master         26h   v1.13.4+c3617b99f   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m4.xlarge,beta.kubernetes.io/os
=linux,failure-domain.beta.kubernetes.io/region=us-east-2,failure-domain.beta.kubernetes.io/zone=us-east-2b,kubernetes.io/hostname=ip-10-0-156-129,node-role.kubernetes.io/master=,node.opens
hift.io/os_id=rhcos,node.openshift.io/os_version=4.1
ip-10-0-167-6.us-east-2.compute.internal     Ready    master         26h   v1.13.4+c3617b99f   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m4.xlarge,beta.kubernetes.io/os
=linux,failure-domain.beta.kubernetes.io/region=us-east-2,failure-domain.beta.kubernetes.io/zone=us-east-2c,kubernetes.io/hostname=ip-10-0-167-6,node-role.kubernetes.io/master=,node.openshi
ft.io/os_id=rhcos,node.openshift.io/os_version=4.1
ip-10-0-170-241.us-east-2.compute.internal   Ready    worker         26h   v1.13.4+c3617b99f   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m4.large,beta.kubernetes.io/os=
linux,failure-domain.beta.kubernetes.io/region=us-east-2,failure-domain.beta.kubernetes.io/zone=us-east-2c,kubernetes.io/hostname=ip-10-0-170-241,node-role.kubernetes.io/worker=,node.opensh
ift.io/os_id=rhcos,node.openshift.io/os_version=4.1
```

- The Machine extension which uses a Kubernetes operator to manage the nodes themselves through the cluster itself;
this can give us a little more information about the nodes and the underlying infrastructure, noting that this environment is running on-top of AWS:

```
$ oc get machines --all-namespaces --show-labels
NAMESPACE               NAME                                         INSTANCE              STATE     TYPE        REGION      ZONE         AGE   LABELS
openshift-machine-api   cluster-3e5f-kqr2b-master-0                  i-02a3979d90b3f67fc   running   m4.xlarge   us-east-2   us-east-2a   26h   machine.openshift.io/cluster-api-cluster=clus
ter-3e5f-kqr2b,machine.openshift.io/cluster-api-machine-role=master,machine.openshift.io/cluster-api-machine-type=master
openshift-machine-api   cluster-3e5f-kqr2b-master-1                  i-051b569b90631f0c7   running   m4.xlarge   us-east-2   us-east-2b   26h   machine.openshift.io/cluster-api-cluster=clus
ter-3e5f-kqr2b,machine.openshift.io/cluster-api-machine-role=master,machine.openshift.io/cluster-api-machine-type=master
openshift-machine-api   cluster-3e5f-kqr2b-master-2                  i-0faba240c04c89423   running   m4.xlarge   us-east-2   us-east-2c   26h   machine.openshift.io/cluster-api-cluster=clus
ter-3e5f-kqr2b,machine.openshift.io/cluster-api-machine-role=master,machine.openshift.io/cluster-api-machine-type=master
openshift-machine-api   cluster-3e5f-kqr2b-worker-us-east-2a-zp8zx   i-087984538c65298ba   running   m4.large    us-east-2   us-east-2a   96m   machine.openshift.io/cluster-api-cluster=clus
ter-3e5f-kqr2b,machine.openshift.io/cluster-api-machine-role=worker,machine.openshift.io/cluster-api-machine-type=worker,machine.openshift.io/cluster-api-machineset=cluster-3e5f-kqr2b-worke
r-us-east-2a
openshift-machine-api   cluster-3e5f-kqr2b-worker-us-east-2b-kbdvm   i-0b401fa18f126f628   running   m4.large    us-east-2   us-east-2b   26h   machine.openshift.io/cluster-api-cluster=clus
ter-3e5f-kqr2b,machine.openshift.io/cluster-api-machine-role=worker,machine.openshift.io/cluster-api-machine-type=worker,machine.openshift.io/cluster-api-machineset=cluster-3e5f-kqr2b-worke
r-us-east-2b
openshift-machine-api   cluster-3e5f-kqr2b-worker-us-east-2c-m9zgh   i-0314b11c4c7d0a89f   running   m4.large    us-east-2   us-east-2c   26h   machine.openshift.io/cluster-api-cluster=clus
ter-3e5f-kqr2b,machine.openshift.io/cluster-api-machine-role=worker,machine.openshift.io/cluster-api-machine-type=worker,machine.openshift.io/cluster-api-machineset=cluster-3e5f-kqr2b-worke
r-us-east-2c
```

In both of these outputs you'll note that both of the outputs have roles listed, and these are associated to kubernetes labels for scheduling purposes; we have two types of role defined: master and worker.
With the exception of master nodes (due to specific scheduling and deployment limitations), all other node types are deployed as part of a MachineSet, and can therefore be scaled as one.

=== More MachineSet Details

In the previous section, you explored the MachineSets resource and scaled the cluster by changing its replica count, adding additional workers, we also configured an auto scaler to ensure that the cluster would have the required capacity to accommodate the workload demand. You'll note that right now we only have worker type machine sets configured:

```
$ oc get machinesets -n openshift-machine-api
NAME                                   DESIRED   CURRENT   READY   AVAILABLE   AGE
cluster-3e5f-kqr2b-worker-us-east-2a   1         1         1       1           26h
cluster-3e5f-kqr2b-worker-us-east-2b   1         1         1       1           26h
cluster-3e5f-kqr2b-worker-us-east-2c   1         1         1       1           26h
```

- Here we have three distinct machine sets deployed, each with a single machine running. Each machine set is aligned with a given AWS EC2 availability zone, and we could easily deploy additional machines (or nodes) into one of the three listed (2a, 2b and 2c). If we wanted to add additional capacity to our cluster we could adjust the replica count like we did in the previous lab, remembering that we randomly selected one of the machine sets in the overall configuration, but that's just for worker nodes, i.e. where our user applications run, and where some of the key infrastructure components run by default, e.g. our routers, etc.

- If we wanted to create a dedicated infrastructure role where we could run specific infrastructure services, components, and pods on (and move them away from the worker nodes like they are by default). We would need to:
  1. Create an additional set of nodes
  2. Define a MachineSet to deploy and scale them into
  3. Label them with specific kubernetes labels.
  4. Configure the various components to run specifically on nodes with those labels.

- To accomplish this, you will create additional MachineSets. The easiest way to do this is:
  1. Get the existing MachineSets by downloading it into a file
  2. Modifying them. This is because the MachineSet has some details that are specific to the AWS region that the cluster is deployed in, like the AWS EC2 AMI ID, so crafting it by hand would be very difficult.

- Here is what we are going to do:
  1. Take a look at one of our MachineSets in detail to understand how the configuration is set
  2. Look to adapt it to create a new one for specifically for our infrastructure services.
  3. Use the following command, noting that you'll have to adjust the command to suit the name of your machine set. You can pick one of the machine sets from previous command:

```
[~] $oc get machineset cluster-3e5f-kqr2b-worker-us-east-2a -n openshift-machine-api -o yaml
```

- Which will give you the following output:

```
[~] $ oc get machineset cluster-3e5f-kqr2b-worker-us-east-2a -n openshift-machine-api -o yaml

apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  annotations:
    autoscaling.openshift.io/machineautoscaler: openshift-machine-api/autoscale-us-east-2a-ts7rr
    machine.openshift.io/cluster-api-autoscaler-node-group-max-size: "4"
    machine.openshift.io/cluster-api-autoscaler-node-group-min-size: "1"
  creationTimestamp: "2019-05-13T20:34:26Z"
  generation: 9
  labels:
    machine.openshift.io/cluster-api-cluster: cluster-3e5f-kqr2b
  name: cluster-3e5f-kqr2b-worker-us-east-2a
  namespace: openshift-machine-api
  resourceVersion: "446823"
  selfLink: /apis/machine.openshift.io/v1beta1/namespaces/openshift-machine-api/machinesets/cluster-3e5f-kqr2b-worker-us-east-2a
  uid: 80644a16-75be-11e9-bb7c-02f7ee4a116e
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: cluster-3e5f-kqr2b
      machine.openshift.io/cluster-api-machine-role: worker
      machine.openshift.io/cluster-api-machine-type: worker
      machine.openshift.io/cluster-api-machineset: cluster-3e5f-kqr2b-worker-us-east-2a
  template:
    metadata:
```

There are a few very important sections in the output, we'll discuss them in depth below...

==== Metadata

The metadata on the MachineSet itself includes information like the name of the MachineSet and various labels:

```
metadata:
  annotations:
    autoscaling.openshift.io/machineautoscaler: openshift-machine-api/autoscale-us-east-2a-ts7rr
    machine.openshift.io/cluster-api-autoscaler-node-group-max-size: "4"
    machine.openshift.io/cluster-api-autoscaler-node-group-min-size: "1"
  creationTimestamp: "2019-05-13T20:34:26Z"
  generation: 9
  labels:
    machine.openshift.io/cluster-api-cluster: cluster-3e5f-kqr2b
  name: cluster-3e5f-kqr2b-worker-us-east-2a
  namespace: openshift-machine-api
  resourceVersion: "446823"
  selfLink: /apis/machine.openshift.io/v1beta1/namespaces/openshift-machine-api/machinesets/cluster-3e5f-kqr2b-worker-us-east-2a
  uid: 80644a16-75be-11e9-bb7c-02f7ee4a116e
```

NOTE: You might see some annotations on your MachineSet if you use the MachineSet that you defined a MachineAutoScaler on in the previous lab section.

==== Selector

```
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: cluster-3e5f-kqr2b
      machine.openshift.io/cluster-api-machine-role: worker
      machine.openshift.io/cluster-api-machine-type: worker
      machine.openshift.io/cluster-api-machineset: cluster-3e5f-kqr2b-worker-us-east-2a
```

In this case, the cluster name is 3e5f-kqr2b and there is an additional label for the whole set.

==== Template Metadata

The template section is the part of the MachineSet that specifically templates out the Machine. The template itself can have metadata associated, and we need to make sure that things match here when we make changes:

```
template:
    metadata:
      creationTimestamp: null
      labels:
        machine.openshift.io/cluster-api-cluster: cluster-3e5f-kqr2b
        machine.openshift.io/cluster-api-machine-role: worker
        machine.openshift.io/cluster-api-machine-type: worker
        machine.openshift.io/cluster-api-machineset: cluster-3e5f-kqr2b-worker-us-east-2a
```

==== Template Spec

The template needs to specify how the Machine/node should be created, i.e. "use this configuration for all machines in this set"; this configuration will be used when provisioning new systems when scaling is required. You will notice that the spec and, more specifically, the providerSpec contains all of the important AWS data to help get the Machine created correctly and bootstrapped.

In our case, we want to ensure that the resulting node inherits one or more specific labels. As you've seen in the examples above, labels go in metadata sections:

```
spec:
      metadata:
        creationTimestamp: null
      providerSpec:
        value:
          ami:
            id: ami-02200f690a88f0819
          apiVersion: awsproviderconfig.openshift.io/v1beta1
          blockDevices:
          - ebs:
              iops: 0
              volumeSize: 120
              volumeType: gp2
          credentialsSecret:
            name: aws-cloud-credentials
          deviceIndex: 0
          iamInstanceProfile:
            id: cluster-3e5f-kqr2b-worker-profile
          instanceType: m4.large
          kind: AWSMachineProviderConfig
          metadata:
            creationTimestamp: null
          placement:
            availabilityZone: us-east-2a
            region: us-east-2
          publicIp: null
          securityGroups:
          - filters:
            - name: tag:Name
              values:
              - cluster-3e5f-kqr2b-worker-sg
          subnet:
            filters:
            - name: tag:Name
              values:
              - cluster-3e5f-kqr2b-private-us-east-2a
          tags:
          - name: kubernetes.io/cluster/cluster-3e5f-kqr2b
            value: owned
          userDataSecret:
            name: worker-user-data
```

By default the MachineSets that the installer creates do not apply any additional labels to the node.

NOTE: As you can probably see, there's plenty of AWS-specific provider configuration here, in future versions of OpenShift, there will be similar respective parameters for other infrastructure providers that can be used.

==== Defining a Custom MachineSet

In this section we're going to be defining a custom MachineSet for infrastructure services. Now that you've inspected an existing MachineSet it's time to go over the rules for creating one, at least for a simple change like we're making:

- Don't change anything in the providerSpec
- Don't change any instances of sigs.k8s.io/cluster-api-cluster: <clusterid>
- Give your MachineSet a unique name
- Make sure any instances of sigs.k8s.io/cluster-api-machineset match the name
- Add labels you want on the nodes to .spec.template.spec.metadata.labels
- Even though you're changing MachineSet name references, be sure not to change the subnet.

This sounds complicated, so let's go through an example. Go ahead and dump one of your existing MachineSets to a file, remembering to adjust this command to match one of yours:

```
$ oc get machineset cluster-3e5f-kqr2b-worker-us-east-2a -o yaml -n openshift-machine-api > infra-machineset.yaml
```

Now open it with a text editor of your choice:

```
$ vi infra-machineset.yaml
```

Let's now take some steps to adapt this MachineSet to suit our required new infrastructure node type...

==== Clean it

Since we asked OpenShift to tell us about an existing MachineSet, there's a lot of extra data that we can immediately remove from the file. Remove the following:

- Within the .metadata top level, remove:

    * generation
    * resourceVersion
    * selfLink
    * uid

- The entire .status block.

- All instances of creationTimestamp.

==== Name It

Go ahead and change the top-level .metadata.name to something indicative of the purpose of this set, for example:

```
name: infrastructure-ap-east-2a
(or anything you name it)
```

By looking at this MachineSet, we can tell that it houses infrastructure-focused Machines (nodes) in ap-east-2 region in the availability zone. Ultimately, you can call this anything you like, but we should change this to something that makes sense for your cluster.

==== Match It
Change any instance of sigs.k8s.io/cluster-api-machineset to match your new name of infrastructure-ap-east-2a (or whatever you're using). This appears in both .spec.selector.matchLabels as well as .spec.template.metadata.labels.

===== Add Your Node Label
Add a labels section to .spec.template.spec.metadata with the label node-role.kubernetes.io/infra: "". Why this particular label? Because oc get node looks at the node-role.kubernetes.io/xxx label and shows that in the output. This will make it easy to identify which workers are also infrastructure nodes (the quotes are because of the boolean).

Your resulting section should look somewhat like the following, albeit with slightly different names as per your unique cluster name:

```
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: cluster-3e5f-kqr2b
      machine.openshift.io/cluster-api-machine-role: worker
      machine.openshift.io/cluster-api-machine-type: worker
      machine.openshift.io/cluster-api-machineset: cluster-3e5f-kqr2b-worker-us-east-2a
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: cluster-3e5f-kqr2b
        machine.openshift.io/cluster-api-machine-role: worker
        machine.openshift.io/cluster-api-machine-type: worker
        machine.openshift.io/cluster-api-machineset: cluster-3e5f-kqr2b-worker-us-east-2a
    spec:
      metadata:
        labels:
          node-role.kubernetes.io/infra: ""
```

==== Set the replica count

For now, make the replica count 1, which it should be already, unless you didn't change it from a previous lab instruction:

```
spec:
  replicas: 1
```

==== Change the Instance Type

If you want a different EC2 instance type, you can change that. It is one of the few things in the providerSpec block you can realistically change. You can also change volumes if you want a different storage size or need additional volumes on your instances.

Save your file and exit.

==== Double Check

Your cluster will have a different ID and you are likely operating in a different version, however, your file should more or less look like the following:

Here is an example of a working infra-machineset.yaml:

```
[~] $ cat infra-machineset.yaml

apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: cluster-4c7b-lkw4d
  name: infra-us-east-2a
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: cluster-4c7b-lkw4d
      machine.openshift.io/cluster-api-machine-role: worker
      machine.openshift.io/cluster-api-machine-type: worker
      machine.openshift.io/cluster-api-machineset: infra-us-east-2a
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: cluster-4c7b-lkw4d
        machine.openshift.io/cluster-api-machine-role: worker
        machine.openshift.io/cluster-api-machine-type: worker
        machine.openshift.io/cluster-api-machineset: infra-us-east-2a
    spec:
      metadata:
        labels:
          node-role.kubernetes.io/infra: ""
      providerSpec:
        value:
          ami:
            id: ami-02200f690a88f0819
          apiVersion: awsproviderconfig.openshift.io/v1beta1
          blockDevices:
          - ebs:
              iops: 0
              volumeSize: 120
              volumeType: gp2
          credentialsSecret:
            name: aws-cloud-credentials
          deviceIndex: 0
          iamInstanceProfile:
            id: cluster-4c7b-lkw4d-worker-profile
          instanceType: m4.large
          kind: AWSMachineProviderConfig
          metadata:
            creationTimestamp: null
          placement:
            availabilityZone: us-east-2a
            region: us-east-2
          publicIp: null
          securityGroups:
          - filters:
            - name: tag:Name
              values:
              - cluster-4c7b-lkw4d-worker-sg
          subnet:
            filters:
            - name: tag:Name
              values:
              - cluster-4c7b-lkw4d-private-us-east-2a
          tags:
          - name: kubernetes.io/cluster/cluster-4c7b-lkw4d
            value: owned
          userDataSecret:
            name: worker-user-data
      versions:
        kubelet: ""
```

=== Create Your Machineset

- Now you can create your MachineSet from the definition that we created:

```
$ oc create -f infra-machineset.yaml -n openshift-machine-api
```
- Then go ahead and check to see if this new MachineSet is listed:

```
[~] $ oc get machineset -n openshift-machine-api
NAME                                   DESIRED   CURRENT   READY   AVAILABLE   AGE
cluster-3e5f-kqr2b-worker-us-east-2a   1         1         1       1           32h
cluster-3e5f-kqr2b-worker-us-east-2b   1         1         1       1           32h
cluster-3e5f-kqr2b-worker-us-east-2c   1         1         1       1           32h
infrastructure-ap-east-2a              1         1                             46s
```

- We don't yet have any ready or available machines in the set because the instance is still coming up and bootstrapping. We can check every minute or to see see whether the machine has been created or not, noting that in the output below the new node is now running:

```
$ oc get machine -n openshift-machine-api
NAME                                         INSTANCE              STATE     TYPE        REGION      ZONE         AGE
cluster-3e5f-kqr2b-master-0                  i-02a3979d90b3f67fc   running   m4.xlarge   us-east-2   us-east-2a   32h
cluster-3e5f-kqr2b-master-1                  i-051b569b90631f0c7   running   m4.xlarge   us-east-2   us-east-2b   32h
cluster-3e5f-kqr2b-master-2                  i-0faba240c04c89423   running   m4.xlarge   us-east-2   us-east-2c   32h
cluster-3e5f-kqr2b-worker-us-east-2a-zp8zx   i-087984538c65298ba   running   m4.large    us-east-2   us-east-2a   7h46m
cluster-3e5f-kqr2b-worker-us-east-2b-kbdvm   i-0b401fa18f126f628   running   m4.large    us-east-2   us-east-2b   32h
cluster-3e5f-kqr2b-worker-us-east-2c-m9zgh   i-0314b11c4c7d0a89f   running   m4.large    us-east-2   us-east-2c   32h
infrastructure-ap-east-2a-2swqt              i-0c68084ced1b9427b   running   m4.large    us-east-2   us-east-2a   20h
```

- Now we can use oc get nodes to see when the actual node is joined and ready. If you're having trouble figuring out which node is the new one, take a look at the AGE column. It will be the youngest! Again, this node may show up as a Machine in the previous API call, but may not have joined the cluster yet, so give it some time to bootstrap properly.

```
$ oc get nodes
NAME                                         STATUS   ROLES          AGE     VERSION
ip-10-0-128-248.us-east-2.compute.internal   Ready    worker         7h46m   v1.13.4+c3617b99f
ip-10-0-137-106.us-east-2.compute.internal   Ready    infra,worker   20h     v1.13.4+c3617b99f
ip-10-0-137-97.us-east-2.compute.internal    Ready    master         32h     v1.13.4+c3617b99f
ip-10-0-148-106.us-east-2.compute.internal   Ready    worker         32h     v1.13.4+c3617b99f
ip-10-0-156-129.us-east-2.compute.internal   Ready    master         32h     v1.13.4+c3617b99f
ip-10-0-167-6.us-east-2.compute.internal     Ready    master         32h     v1.13.4+c3617b99f
ip-10-0-170-241.us-east-2.compute.internal   Ready    worker         32h     v1.13.4+c3617b99f
```

==== Check the Labels

- In our case, the youngest node was named ip-10-0-137-106.us-east-2.compute.internal, so we can ask what its labels are:

```
$ oc get node ip-10-0-137-106.us-east-2.compute.internal --show-labels
NAME                                         STATUS   ROLES          AGE   VERSION             LABELS
ip-10-0-137-106.us-east-2.compute.internal   Ready    infra,worker   20h   v1.13.4+c3617b99f   beta.kubernetes.io/arch=amd64,beta.kubernetes.io/instance-type=m4.large,
beta.kubernetes.io/os=linux,failure-domain.beta.kubernetes.io/region=us-east-2,failure-domain.beta.kubernetes.io/zone=us-east-2a,kubernetes.io/hostname=ip-10-0-137-106
,node-role.kubernetes.io/infra=,node-role.kubernetes.io/worker=,node.openshift.io/os_id=rhcos,node.openshift.io/os_version=4.1
```

- It's hard to see, but our node-role.kubernetes.io/infra label is the LABELS column. You will also see infra,worker in the output of oc get node in the ROLES column. Success!

==== Add More Machinesets (or scale, or both)

- In a realistic production deployment, you would want at least 3 MachineSets to hold infrastructure components. Both the logging aggregation solution and the service mesh will deploy ElasticSearch, and ElasticSearch really needs 3 instances spread across 3 discrete nodes. Why 3 MachineSets? Well, in theory, having a MachineSet in different AZs ensures that you don't go completely dark if AWS loses an AZ.
For the purposes of this exercise, though, we'll just scale up our single set:

```
$ oc edit machineset infrastructure-ap-east-2a -n openshift-machine-api
(Opens in vi)
```

NOTE: If you're uncomfortable with vi(m) you can use your favorite editor by specifying EDITOR=<your choice> before the oc command.

- Change the .spec.replicas from 1 to 3, and then save/exit the editor.

- Execute the following commands:
  1. `oc project openshift-machine-api`
  2. `oc get machineset` to see the change in the desired number of instances
  3. `oc get machine`
  4. `oc get node`

=== Moving Infrastructure Components

Now that we have provisioned some infrastructure specific nodes, it's time to move various infrastructure components onto them, i.e. move them away from the worker nodes, and onto the fresh systems. Let's go through some of them individually to see how they can be moved, and how to monitor the progress.

==== Router

- The OpenShift router is deployed, maintained, and scaled by an Operator called openshift-ingress-operator. Its Pod lives in the openshift-ingress-operator project:

```
$ oc get pod -n openshift-ingress-operator
NAME                               READY   STATUS    RESTARTS   AGE
ingress-operator-5895456c5-vwnc6   1/1     Running   0          32h
```

- The actual default router instance lives in the openshift-ingress project:

```
$ oc get pod -n openshift-ingress -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP           NODE                                         NOMINATED NODE   READINESS GATES
router-default-7db478d879-bzwws   1/1     Running   0          20h   10.131.4.4   ip-10-0-128-248.us-east-2.compute.internal   <none>           <none>
router-default-7db478d879-nwftw   1/1     Running   0          20h   10.130.4.4   ip-10-0-170-241.us-east-2.compute.internal   <none>           <none>
```

- The cluster deploys two routers for availability and fault tolerance, and you can see that the pods are deployed across two nodes. Right now, these will be deployed on nodes with the worker label, and not on the infrastructure nodes that were recently deployed, as the default configuration of the router operator is to pick nodes with the role of worker.

Pick one of the nodes (from NODE) where a router pod is running and see the ROLES column:

```
$ oc get node ip-10-0-170-241.us-east-2.compute.internal
NAME                                         STATUS   ROLES    AGE   VERSION
ip-10-0-170-241.us-east-2.compute.internal   Ready    worker   32h   v1.13.4+c3617b99f
```

- now that we have created dedicated infrastructure nodes, we want to tell the operator to put the router instances on nodes with the new role of infra.

- The OpenShift router operator creates a custom resource definition (CRD) called ingresscontroller. The ingresscontroller objects are observed by the router operator and tell the operator how to create and configure routers. Let's take a look:

```
$ oc get ingresscontroller default -n openshift-ingress-operator -o yaml

apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: "2019-05-13T20:39:27Z"
  finalizers:
  - ingresscontroller.operator.openshift.io/finalizer-ingresscontroller
  generation: 2
  name: default
  namespace: openshift-ingress-operator
  resourceVersion: "199439"
  selfLink: /apis/operator.openshift.io/v1/namespaces/openshift-ingress-operator/ingresscontrollers/default
  uid: 33c90a62-75bf-11e9-a65b-02affe1c7e26
spec:
  nodePlacement:
    nodeSelector:
      matchLabels:
        node-role.kubernetes.io/worker: ""
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: "2019-05-13T20:41:27Z"
    status: "True"
    type: Available
  domain: apps.cluster-3e5f.sandbox580.opentlc.com
  endpointPublishingStrategy:
    type: LoadBalancerService
  selector: ingresscontroller.operator.openshift.io/deployment-ingresscontroller=default
```

- As you can see, the nodeSelector is configured for the worker role. Go ahead and use oc edit to change node-role.kubernetes.io/worker to be node-role.kubernetes.io/infra:

```
$ oc edit ingresscontroller default -n openshift-ingress-operator -o yaml
(Opens in vi)
```

- The relevant section should look like the following:


```
spec:
  nodePlacement:
    nodeSelector:
      matchLabels:
        node-role.kubernetes.io/infra: ""
```

- After saving and exiting the editor, if you're quick enough, you might catch the router pod being moved to its new home. Run the following command and you may see something like:

```
$ oc get pod -n openshift-ingress -o wide
NAME                              READY     STATUS        RESTARTS   AGE       IP           NODE                                              NOMINATED NODE
router-default-5fc6c9ffbb-9x9l8   1/1       Running       0          15h       10.131.4.4   ip-10-0-139-255.us-east-2.compute.internal        <none>
router-default-5fc6c9ffbb-p5x6d   0/1       Terminating   0          15h       10.131.4.4   ip-10-0-128-248.us-east-2.compute.internal        <none>
```

- In the above output, the Terminating pod was running on one of the worker nodes. The Running pod is now on one of our nodes

NOTE: The actual moving of the pod is currently not working (you can track the progress here), so as a temporary workaround we can force the router pods to be rebuilt on other nodes by running:

```
$ for i in $(oc get pod -n openshift-ingress | awk 'NR>1{print $1;}'); do oc delete pod $i -n openshift-ingress; done
pod "router-default-5fc6c9ffbb-9x9l8" deleted
pod "router-default-5fc6c9ffbb-p5x6d" deleted
```

WARNING: If you're using the browser-based terminal, your session will hang when the router pods get removed, as we're reliant on the routers to serve your console. The session may restore itself after a minute or two, or you can try reloading the page.

- If we wait a minute or so, we should see that the pods are rebuilt:

```
$ oc get pod -n openshift-ingress -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP           NODE                                         NOMINATED NODE   READINESS GATES
router-default-7db478d879-bzwws   1/1     Running   0          22h   10.131.4.4   ip-10-0-139-255.us-east-2.compute.internal   <none>           <none>
router-default-7db478d879-nwftw   1/1     Running   0          22h   10.130.4.4   ip-10-0-137-106.us-east-2.compute.internal   <none>           <none>
```

- If we check one of the nodes for the ROLE that it's labeled with:

```
$ oc get node ip-10-0-139-255.us-east-2.compute.internal
NAME                                         STATUS   ROLES          AGE   VERSION
ip-10-0-139-255.us-east-2.compute.internal   Ready    infra,worker   22h   v1.13.4+c3617b99f
```

Success! Our pods have been automatically redeployed onto the infrastructure nodes.

==== Container Image Registry

The registry uses a similar CRD (Custom Resource Definition) mechanism to configure how the operator deploys the actual registry pods. That CRD is configs.imageregistry.operator.openshift.io. You will need to edit the cluster CR object in order to add the nodeSelector.

- First, take a look at it:

```
[~] $ oc get configs.imageregistry.operator.openshift.io/cluster -o yaml

apiVersion: imageregistry.operator.openshift.io/v1
kind: Config
metadata:
  creationTimestamp: "2019-05-13T20:39:22Z"
  finalizers:
  - imageregistry.operator.openshift.io/finalizer
  generation: 3
  name: cluster
  resourceVersion: "200927"
  selfLink: /apis/imageregistry.operator.openshift.io/v1/configs/cluster
  uid: 3077588d-75bf-11e9-8ad1-0af01fb55bd2
spec:
  defaultRoute: false
  httpSecret: 66b879954287368617ed5165caff19ebd07d2dabe4edb84509875623b9ff07914de72f832d4e80bb993d18220e935a65ce3b30e29eaf170f645b2d2e4a65a2c0
  logging: 2
  managementState: Managed
  proxy:
    http: ""
    https: ""
    noProxy: ""
  readOnly: false
  replicas: 1
  requests:
    read:
      maxInQueue: 0
      maxRunning: 0
      maxWaitInQueue: 0s
    write:
      maxInQueue: 0
      maxRunning: 0
      maxWaitInQueue: 0s
(...)
```

- Next, let's modify the custom resource by live-patching the configuration. For this we can use oc edit, and you'll need to modify the .spec section:

```
[~] $ oc edit configs.imageregistry.operator.openshift.io/cluster
```

- The .spec section will need to look like the following:

```
  nodeSelector:
    node-role.kubernetes.io/infra: ""
```

- Once you're done, save and exit the editor, and it should confirm the change:

```
config.imageregistry.operator.openshift.io/cluster edited
```

NOTE: The nodeSelector stanza may be added anywhere inside the .spec block.

- When you save and exit you should see the registry pod being moved to the infra node. The registry is in the openshift-image-registry project. If you execute the following quickly enough, you may see the old registry pods terminating and the new ones starting.:

```
[~] $ oc get pod -n openshift-image-registry
NAME                                               READY   STATUS        RESTARTS   AGE
cluster-image-registry-operator-5644775d7c-w78kh   1/1     Running       0          34h
image-registry-5878c9d896-nmkc6                    1/1     Terminating   0          22h
node-ca-2ljck                                      1/1     Running       0          22h
node-ca-9npbz                                      1/1     Running       0          34h
node-ca-mk9lj                                      1/1     Running       0          34h
node-ca-pspwx                                      1/1     Running       0          34h
node-ca-qlxqx                                      1/1     Running       0          9h
node-ca-qvslw                                      1/1     Running       0          34h
node-ca-wxb55                                      1/1     Running       0          34h
node-ca-xn9vg                                      1/1     Running       0          22h
```

NOTE: At this time the image registry is not using a separate project for its operator. Both the operator and the operand are housed in the openshift-image-registry project.

- Since the registry is being backed by an S3 bucket, it doesn't matter what node the new registry pod instance lands on. It's talking to an object store via an API, so any existing images stored there will remain accessible.

- Also note that the default replica count is 1. In a real-world environment you might wish to scale that up for better availability, network throughput, or other reasons.

- If you look at the node on which the registry landed (noting that you'll likely have to refresh your list of pods by using the previous commands to get its new name):

```
[~] $ oc get pod image-registry-5878c9d896-nmkc6 -n openshift-image-registry -o wide
NAME                              READY   STATUS    RESTARTS   AGE   IP           NODE                                         NOMINATED NODE   READINESS GATES
image-registry-5878c9d896-nmkc6   1/1     Running   0          22h   10.131.4.5   ip-10-0-139-255.us-east-2.compute.internal   <none>           <none>
```

...you'll note that it is now running on an infra worker:

```
[~] $ oc get node ip-10-0-139-255.us-east-2.compute.internal
NAME                                         STATUS   ROLES          AGE   VERSION
ip-10-0-139-255.us-east-2.compute.internal   Ready    infra,worker   22h   v1.13.4+c3617b99f
```

Lastly, notice that the CRD for the image registry's configuration is not namespaced -- it is cluster scoped. There is only one internal/integrated registry per OpenShift cluster that serves all projects.

==== Monitoring

The Cluster Monitoring operator is responsible for deploying and managing the state of the Prometheus+Grafana+AlertManager cluster monitoring stack. It is installed by default during the initial cluster installation. Its operator uses a ConfigMap in the openshift-monitoring project to set various tunables and settings for the behavior of the monitoring stack.

- There is no ConfigMap created as part of the installation. Without one, the operator will assume default settings, as we can see, this is not defined:

```
[~] $ oc get configmap cluster-monitoring-config -n openshift-monitoring
NAME                        DATA   AGE
cluster-monitoring-config   1      22h
```

- Even with the default settings, The operator will create several ConfigMap objects for the various monitoring stack components, and you can see them, too:

```
[~] $ oc get configmap -n openshift-monitoring
NAME                                                  DATA   AGE
adapter-config                                        1      34h
cluster-monitoring-config                             1      22h
grafana-dashboard-etcd                                1      34h
grafana-dashboard-k8s-cluster-rsrc-use                1      34h
grafana-dashboard-k8s-node-rsrc-use                   1      34h
grafana-dashboard-k8s-resources-cluster               1      34h
grafana-dashboard-k8s-resources-namespace             1      34h
grafana-dashboard-k8s-resources-pod                   1      34h
grafana-dashboard-k8s-resources-workload              1      34h
grafana-dashboard-k8s-resources-workloads-namespace   1      34h
grafana-dashboards                                    1      34h
kubelet-serving-ca-bundle                             1      34h
prometheus-adapter-prometheus-config                  1      34h
prometheus-k8s-rulefiles-0                            1      34h
serving-certs-ca-bundle                               1      34h
sharing-config                                        3      34h
telemeter-client-serving-certs-ca-bundle              1      34h
```

- Take a look at the following file, it contains the definition for a ConfigMap that will cause the monitoring solution to be redeployed onto infrastructure nodes:

https://github.com/openshift/training/blob/master/assets/cluster-monitoring-configmap.yaml

- Let's use this as our new configuration; you can create the new monitoring config with the following command:

```
[~] $ oc create -f https://raw.githubusercontent.com/openshift/training/master/assets/cluster-monitoring-configmap.yaml
configmap/cluster-monitoring-config created
```

- We can now watch the various monitoring pods be redeployed onto our infrastructure nodes with the following command:

```
[~] $ oc get pod -w -n openshift-monitoring
NAME                                           READY     STATUS              RESTARTS   AGE
alertmanager-main-0                            3/3       Running             0          16h
alertmanager-main-1                            3/3       Running             0          16h
alertmanager-main-2                            0/3       ContainerCreating   0          3s
cluster-monitoring-operator-6fc8c9bc75-6pfpw   1/1       Running             0          16h
grafana-574679769d-7f9mf                       2/2       Running             0          16h
kube-state-metrics-55f8d66c77-sbbbc            3/3       Running             0          16h
kube-state-metrics-578dbdf85d-85vm7            0/3       ContainerCreating   0          9s
node-exporter-2x7b7                            2/2       Running             0          16h
node-exporter-d4vq9                            2/2       Running             0          45m
node-exporter-dx5kz                            2/2       Running             0          16h
node-exporter-f9g4h                            2/2       Running             0          16h
node-exporter-kvd5x                            2/2       Running             0          45m
node-exporter-ntzbp                            2/2       Running             0          16h
node-exporter-prsj9                            2/2       Running             0          1h
node-exporter-qx9lf                            2/2       Running             0          16h
node-exporter-wh9qs                            2/2       Running             0          16h
prometheus-adapter-7fb8c8b544-jn8q2            1/1       Running             0          32m
prometheus-adapter-7fb8c8b544-v5rfs            1/1       Running             0          33m
prometheus-k8s-0                               6/6       Running             1          16h
prometheus-k8s-1                               6/6       Running             1          16h
prometheus-operator-7787679668-nxc6s           0/1       ContainerCreating   0          8s
prometheus-operator-954644495-m64hd            1/1       Running             0          16h
telemeter-client-79f99d7bc6-4p8zv              3/3       Running             0          16h
telemeter-client-7f48f48dd7-dvblb              0/3       ContainerCreating   0          4s
grafana-5fc5979587-bdkcd                       0/2       Pending             0          3s

(Ctrl+C to exit)
```

NOTE: You can also run watch 'oc get pod -n openshift-monitoring' as an alternative.

Congratulations!! You now know how to set up infrastructure nodes on OpenShift 4 cluster!! For more information, see https://docs.openshift.com/container-platform/4.1/machine_management/creating-infrastructure-machinesets.html.
